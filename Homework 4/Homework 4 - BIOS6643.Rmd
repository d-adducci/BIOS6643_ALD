---
title: "Homework 4 - BIOS 6643 Analysis of Longitudinal Data"
author: "Dominic Adducci"
date: "2023-09-20"
output:
  pdf_document: default
---

```{r, echo = F, include = F}

library(tidyverse)
library(latex2exp)
library(broom.mixed)
library(kableExtra)
library(knitr)
library(nlme)

```

# Question 1
Complex MLE estimation in LMM requires computational optimization approaches, the goal here is to implement a basic Newton-Raphson algorithm in R. The following data are an i.i.d sample from a Cauchy($\theta$,1) distribution: 1.77, -0.23, 2.76, 3.80, 3.47, 56.75, -1.34, 4.24, -2.44, 3.29, 3.71, -2.40, 4.53, -0.07, -1.05, -13.87, -2.53, -1.75, 0.27, 43.21. 

#### Part A: 
Graph the log likelihood function. 

The likelihood function of the Cauchy($\theta$,1) distribution is: 
$$\frac{1}{\pi^n}\frac{1}{\prod[1+(x_i-\theta)^2]}$$
and the loglikelihood is:
$$-nlog(\pi)-\sum_i^nlog(1+(x_i - \theta)^2)$$

```{r, echo = F}

### START QUESTION 1 CODE ###

## START QUESTION 1 PART A CODE ##

# Initializing experimental data. 
exp_data <- c(1.77,-0.23,2.76,3.80,3.47,56.75,-1.34,4.24,-2.44,3.29,3.71,-2.40,
              4.53,-0.07,-1.05,-13.87,-2.53,-1.75,0.27,43.21)

# Making a function to calculate the loglikelihood for the Cauchy distribution
# which has scale parameter set at 1. The location parameter is allowed to vary.
cauchy_function <- function(exp_data,theta){
  ll_prob <- -length(exp_data)*log(pi) - sum(log(1+(exp_data-theta)^2))
  output <- c(theta,ll_prob)
  return(output)
}

# Finding the median of the experimental data to give an idea of range for 
# potential location parameters (comes out to 1.02). 
cauchy_median <- median(exp_data)

# Initializing range of location parameters to check. Because Cauchy is 
# continuous thetas increase at increments of 1/30. Median of experimental 
# data is 1 so I'll include a range from -10 to 10. 
theta_range <- seq(-20,20,1/30)

# Using an apply statement to enter location parameter range into function.
ll_cauchy <- data.frame(t(sapply(theta_range, 
                               function(x) cauchy_function(exp_data,x))))

colnames(ll_cauchy) <- c("Theta","LogLikelihood")

# Selecting max loglikelihood value.
max_ll <- ll_cauchy[ll_cauchy$LogLikelihood == max(ll_cauchy$LogLikelihood),]

# Making a plot of the loglikelihood. 
ll_cauchy_plot <- ggplot(ll_cauchy, aes(x = Theta,
                                        y = LogLikelihood)) +
  geom_point(size = 0.1) +
  geom_vline(xintercept = max_ll[,1], color = "red") +
  geom_hline(yintercept = max_ll[,2], color = "red") +
  labs(title = TeX("LogLikelihood Cauchy($\\theta$,1)"),
       x = TeX("$\\theta$"), y = "LogLikelihood") +
  expand_limits(y = c(-125,-70)) +
  theme_bw()


ll_cauchy_plot

## END QUESTION 1 PART A CODE ##

```
From the plot of log-likelihood the maximum log-likelihood values is -72.92, and the optimal theta is -0.2. 

# Part B
Find (and write an R program) to find the MLE for $\theta$ using the Newton-Raphson method. 

The equation for the Newton-Raphson method in general form is as follows:
$$x_i = x_{i-1} - \frac{f(x_{i-1})}{f'(x_{i-1})}$$
Translating this to finding the MLE of $\theta$:
$$\hat{\theta}_{i}=\hat{\theta}_{i-1}-\frac{logL'(\hat{\theta}_{i-1})}{logL''(\hat{\theta}_{i-1})}=\hat{\theta}_{i-1}-\frac{S(\hat{\theta}_{i-1})}{I(\hat{\theta}_{i-1})}$$
What finding an MLE the score $S(\theta)$, the derivative of the log-likelihood, should equal 0, $S(\theta)=0$. $I(\theta)$ is the Fisher observed information from the data.

Check the code appendix for the function. 

```{r, echo = F}

## START QUESTION 1 PART B CODE

# For the log likelihood of the Cauchy distribution the first term drops out
# after taking the first derivative, so only the second needs to be included.
ll_cauchy_form <- expression(-log(1+(exp_data-theta)^2))

# Finding first and second derivatives of loglikelihood
# (will copy the output and put it into the function)
first_d <- D(ll_cauchy_form,"theta")
second_d <- D(first_d,"theta")

# Making a function to calculate the Newton-Raphson method.
nr_mle <- function(exp_data,theta_est,loglike,iterations){
  # Initializing a data frame to track progress
  theta_mat <- data.frame(matrix(NA,nrow = iterations,ncol=2))
  theta = theta_est # Initializing theta
  for(i in 1:iterations){
    # Calculating first and second derivative log likelihood
    ll_first_d <- sum(2 * (exp_data - theta)/(1 + (exp_data - theta)^2)) 
    ll_second_d <- sum(-(2/(1 + (exp_data - theta)^2) - 2 * 
                           (exp_data - theta) * 
                           (2 * (exp_data - theta))/
                           (1 + (exp_data - theta)^2)^2)) 
    theta <- theta - ll_first_d/ll_second_d # Using Newton-Raphson equation
    # Calculating difference from original theta. 
    difference <- theta - theta_est 
    # Adding these to the data frame. 
    theta_mat[i,1] <- theta
    theta_mat[i,2] <- difference
  }
  colnames(theta_mat) <- c("Estimate","Difference")
  return(theta_mat)
}


theta_mle <- nr_mle(exp_data,0,ll_cauchy_form,20)
theta_mle

## END QUESTION 1 PART B CODE

### END QUESTION 1 CODE ###

```

#### Part C
Try all of the following starting points: -11,-1,0,1.5,4,4.7,7,8,and 38. 




# Question 2
In a paragraph explain the difference between a general linear model or multiple regression (GLM; not a generalized linear model like a logistic regression or GLM; not a generalized linear model like a logistic regression or Poisson, which will be discussed later) and a linear mixed model (LMM). 

A general linear model only has fixed effects, while a linear mixed model includes both fixed and random effects. The random effects of the linear mixed model allows the model to account for differences between subjects. A simple example would be measuring something like cholesterol levels through time. A general linear model (fixed effects only) may include covariates such as time, BMI, smoking status, sex, race, etc. Every individual will follow the same regression line based on those covariates. In a linear mixed model random effect which account for subject differences, such as someone tending to have higher or lower cholesterol at start (random intercept) can be included to better model change over time. A better fit may be found by including random slopes as well for each subject if cholesterol trajectory is found to be different between subjects. 

# Question 3
In a short paragraph, explain the difference between a profiled likelihood and a restricted likelihood for a linear mixed model, and how and why they are used. 

In a profile likelihood you maximize the likelihood by fixing every other parameter and only allowing one to vary. Doing this maximizes the single parameter you allowed to vary. You can then repeat this process for every other parameter incrementally, plugging in the estimates for parameters which have already been maximized. The downside to this method is that variance estimates are biased downward. The restricted likelihood (REML) allows for estimating parameters which are not biased regardless of sample size. The downside for the REML method compared to profile likelihood is that you can only compare REML model using a likelihood ratio of both models have the same set of fixed effects. 

Profile likelihood - maximize likelihood by fixing every other parameter and only allow one to vary and maximize that using a grid search. After getting that maximum repeat each process for each parameter in the set of likelihood parameter.

Restricted maximum likelihood - REML has property that your standard error are unbiased regardless of sample size, so generally is in situations where were trying to get unbiased estimates of errors when sample sizes are small. Loglikelihood are not valid for reduced in full REML models. Can only compare REML models that have the same set of fixed effects. 


# Part 2ish 

Investigator wants to understand whether Cortisol (a stress hormone) secretion differs in women suffering from depression. Cortisol was measured every 10 minutes for a period of 24 hours starting at 9 am. 26 patients and 26 controls were collected in the study. Although the data were collected every 10 minutes for a period of 24 hours on each subject (144 observations), the investigators were interested in differences in the circadian pattern between the groups. Data was divided into 6 blocks of 4 hours and averaged to obtain a set of "block means". 

```{r, echo = F}

### START PART 2 DATA FORMATTING ###

cort_data <- read.csv("C:/Users/domin/Documents/Biostatistics Masters Program/Fall 2023/BIOS 6643 - ADL/BIOS6643_ALD/Homework 1/DataRaw/cortdata.csv")

cort_data_long <- cort_data %>% pivot_longer(cols = "Time1":"Time6", 
                                             names_to = "time",
                                             values_to = "mean_cortisol")

cort_data_long <- cort_data_long %>% mutate(casecontrol = factor(casecontrol),
                                            time = factor(time))

### END PART 2 DATA FORMATTING ### 

```




# Question 4

#### Part A
Fit a multiple linear regression to investigate how mean cortisol values change over the day (categorical time) and how the average cortisol levels differ by group (no interaction for this model). This will be used to anchor the comparisons later in the assignment. 
```{r, echo = F}

### START QUESTION 4 CODE ###

## START QUESTION 4 PART A CODE ##

# Making a multiple linear regression model for mean cortisol. 
mlr_cort <- lm(mean_cortisol ~ time + casecontrol, data = cort_data_long)

# Formatting output into a table. 
broom::tidy(mlr_cort, conf.int = TRUE) %>%
  mutate(p.value = format.pval(p.value)) %>%
  select(term,estimate,std.error,conf.low,conf.high,p.value) %>%
  kbl(digits = 4, booktabs = T, align = "cc",
      caption = "MLR Cortisol",
      col.names = c("Term","Estimate","Std.Error",
                    "95% Conf.Low","95% Conf.High","P-Value")) %>%
  kable_styling(latex_options = "HOLD_position")

## END QUESTION 4 PART A CODE ##
  

```
Table 1 shows the output of the multiple linear regression where mean cortisol is the outcome and time and casecontrol are covariates. Time is factored into 6 different levels, where time 1 is the reference. 

#### Part B
Provide a table of mean differences from the 6th time period along with SE's of the differences. Interpret two of the coefficients. You do not need to conduct inference.
```{r, echo = F}

## START QUESTION 4 PART B CODE ##

# Releveling the time factor variable so time 6 is the reference group.
cort_long_relevel <- cort_data_long %>% mutate(time = relevel(time,ref = 6))

# Making a multiple linear regression for cortisol 
# where time 6 is the reference.
mlr_cort_relev <- lm(mean_cortisol ~ time + casecontrol, data = cort_long_relevel)

# Formatting output into a table. 
broom::tidy(mlr_cort_relev, conf.int = TRUE) %>%
  mutate(p.value = format.pval(p.value)) %>%
  select(term,estimate,std.error,conf.low,conf.high,p.value) %>%
  kbl(digits = 4, booktabs = T, align = "cc",
      caption = "MLR Cortisol - Time 6 Reference",
      col.names = c("Term","Estimate","Std.Error","95% Conf.Low",
                    "95% Conf.High","P-Value")) %>%
  kable_styling(latex_options = "HOLD_position")

## END QUESTION 4 PART B CODE ##

```
Table 2 shows the output of the multiple linear regression where time 6 is the reference group. For term timeTime1 this is the mean difference between cortisol at time 1 and cortisol at time 6. Interpreting this it means cortisol is 1.7518 units higher at time 6 compared to time 1. For the term casecontrolp this is the mean difference between the control (reference) group and women with depression. Interpreting this it means cortisol is 0.9211 units higher for women with depression compared to women without depression (control). 

#### Part C
Will these standard error be too big or small and why? 

Standard error is found with the equation:
$$Std.Error = \frac{\sigma}{\sqrt{n}}$$
For a study at a single instance n is generally the number of subjects. For this study because subjects each have 6 time points (assuming balanced data) n refers to all instances where cortisol was measured for each subject, $52 \times 6$, which mean $n = 312$. 
```{r, echo = F}

## START QUESTION 4 PART C CODE ##

# Selecting standard errors and converting to variance. 
q4_var <- ((summary(mlr_cort_relev)$coefficients[,2]) * 
  sqrt(length(cort_long_relevel)))^2

q4_var_tbl <- kbl(data.frame(t(q4_var)),
                  caption = "MLR Cortisol - Time 6 Ref. Variances",
                  col.names = c("Intercept","Time 1","Time 2","Time 3",
                                "Time 4","Time 5","Group P"),
                  booktabs = T, align = "cc") %>%
  kable_styling(latex_options = "HOLD_position")

q4_var_tbl

## END QUESTION 4 PART C CODE ##

### END QUESTION 4 CODE ###

```
Table 3 shows the variances for each estimate. Given that each estimate is relatively low compared to the variances the standard errors (which were used to calculate variance) may be too large to make a meaningful interpretation, as subjects may have significantly different mean cortisol measurements between each other. 

# Question 5

#### Part A
Fit a linear mixed model assuming categorical time and group with an unstructured variance-covariance structure using method=REML (the default). Print out the correlation matrix and interpret the general patterns of the correlation in the errors for an individual. 

```{r, echo = F}

### START QUESTION 5 CODE ###

## START QUESTION 5 PART A CODE ##

lmm_unstr_corr <- gls(mean_cortisol ~ time + casecontrol, 
                      data = cort_data_long, 
                      correlation = corSymm(form = ~ 1 | SubjectId))

lmm_unstr_corr_sum <- summary(lmm_unstr_corr)

broom.mixed::tidy(lmm_unstr_corr, conf.int = TRUE) %>%
  mutate(p.value = format.pval(p.value)) %>%
  select(term,estimate,std.error,conf.low,conf.high,p.value) %>%
  kbl(digits = 4, booktabs = T, align = "cc",
      caption = "LMM Unstructured Var-Cov",
      col.names = c("Term","Estimate","Std.Error","95% Conf.Low",
                    "95% Conf. High","P-Value")) %>%
  kable_styling(latex_options = "HOLD_position")

```
Table 4 shows the fixed effects for the linear mixed effects model with unstructured variance-covariance. 
```{r, echo = F}

lmm_unstr_corr_sum$modelStruct$corStruct

## END QUESTION 5 PART A CODE ##

```
The above output shows the correlation structure for the linear mixed model with unstructured variance-covariance. 

#### Part B
Provide a table of mean differences from the 6th time period along with SE's of the differences. Interpret two of the coefficients. You do not need to conduce inference. 
```{r, echo = F}

## START QUESTION 5 PART B CODE ##
lmm_unstr_corr_relev <- gls(mean_cortisol ~ time + casecontrol, 
                      data = cort_long_relevel, 
                      correlation = corSymm(form = ~ 1 | SubjectId))

broom.mixed::tidy(lmm_unstr_corr_relev, conf.int = TRUE) %>%
  select(term,estimate,std.error,conf.low,conf.high,p.value) %>%
  mutate(p.value = format.pval(p.value)) %>%
  kbl(digits = 4, booktabs = T, align = "cc",
      caption = "LMM Unstructured Var-Cov - Time 6 Ref.",
      col.names = c("Term","Estimate","Std.Error","95% Conf.Low",
                    "95% Conf. High","P-Value")) %>%
  kable_styling(latex_options = "HOLD_position")

## END QUESTION 5 PART B CODE ##

### END QUESTION 5 CODE ###

```
Table 5 shows the output of the linear mixed model with unstructured variance-covariance where time 6 is the reference group. 

* For term timeTime1 this is the mean difference between cortisol at time 1 and cortisol at time 6. Interpreting this it means cortisol is 1.7518 units higher at time 6 compared to time 1. This estimate is identical to that of the MLR model from question 4, but the standard error is smaller at 0.3213 compared to the MLR model's standard error of 0.6662. In other words the estimate of the LMM with unstructured variance-covariance is more precise. 

* For term timeTime2 this is the mean difference between cortisol at time 2 and cortisol at time 6. Interpreting this is means cortisol is 2.2179 units higher at time 2 compared to time 6. This estimate is identical to that of the MLR model from question 4, but the standard error is smaller at 0.5658 compared to the MLR model's standard error of 0.6662. In other words the estimate of the LMM with unstructured variance-covariance is more precise. 

#### Part C
What is the estimate difference in mean cortisol levels (and SE) between the depressed and non-depressed groups. Interpret the finding in a sentence. You do not need to conduct inference. 

From table 4 (or 5, they are equivalent for the casecontrol variable) the estimate difference between the control group and the depressed group is 0.7591. This means that cortisol is 0.7591 units higher in the depressed group. The estimate is smaller than the estimate from the MLR model, but has a higher standard error of 0.7254 compared to the MLR model's standard error of 0.3846. 

# Question 6

#### Part A
Fit a linear mixed model assuming categorical time and group with a compound symmetry structure using method=REML (the default). Print out the correlation matrix and interpret the general patterns of the correlation in the errors for an individual. 
```{r, echo = F}

### START QUESTION 6 CODE ###

## START QUESTION 6 PART A CODE ##

lmm_compsymm <- gls(mean_cortisol ~ time + casecontrol, data = cort_data_long,
                    correlation = corCompSymm(form = ~ 1 | SubjectId))

lmm_compsymm_sum <- summary(lmm_compsymm)

broom.mixed::tidy(lmm_compsymm, conf.int = TRUE) %>%
  mutate(p.value = format.pval(p.value)) %>%
  select(term,estimate,std.error,conf.low,conf.high,p.value) %>%
  kbl(digits = 4, booktabs = T, align = "cc",
      caption = "LMM Compound Symmetry Var-Cov",
      col.names = c("Term","Estimate","Std.Error","95% Conf.Low",
                    "95% Conf. High","P-Value")) %>%
  kable_styling(latex_options = "HOLD_position")

```
Table 6 shows the fixed effects for the linear mixed effects model with compound symmetry variance-covariance structure. 
```{r,echo = F}

lmm_compsymm$modelStruct$corStruct

## END QUESTION 6 PART A CODE ##

```
The above output shows the correlation structure for the linear mixed model with compound symmetry variance-covariance structure. 

#### Part B
Provide a table of mean differences from the 6th time period along with SE's of the differences. Interpret two of the coefficients. You do not need to conduct inference. 
```{r, echo = F}

## START QUESTION 6 PART B CODE ##
lmm_compsymm_relev <- gls(mean_cortisol ~ time + casecontrol, 
                      data = cort_long_relevel, 
                      correlation = corCompSymm(form = ~ 1 | SubjectId))

broom.mixed::tidy(lmm_compsymm_relev, conf.int = TRUE) %>%
  select(term,estimate,std.error,conf.low,conf.high,p.value) %>%
  mutate(p.value = format.pval(p.value)) %>%
  kbl(digits = 4, booktabs = T, align = "cc",
      caption = "LMM Compound Symmetry Var-Cov - Time 6 Ref.",
      col.names = c("Term","Estimate","Std.Error","95% Conf.Low",
                    "95% Conf. High","P-Value")) %>%
  kable_styling(latex_options = "HOLD_position")

## END QUESTION 6 PART B CODE ##

### END QUESTION 6 CODE ###

```
Table 7 shows the output of the linear mixed model with compound symmetry variance-covariance where time 6 is the reference group. Because the variance-covariance structure is compound symmetric each SE for time points are the same. 

* For term timeTime1 this is the mean difference between cortisol at time 1 and cortisol at time 6. Interpreting this it means cortisol is 1.7518 units higher at time 6 compared to time 1. This estimate is identical to that of the MLR model from question 4, but the standard error is smaller at 0.4770 compared to the MLR model's standard error of 0.6662. In other words the estimate of the LMM with compound symmetry variance-covariance is more precise. 
* For term timeTime2 this is the mean difference between cortisol at time 2 and cortisol at time 6. Interpreting this it means cortisol is 2.2179 units higher at time 2 compared to time 6. This estimate is identical to that of the MLR model from question 4, but the standard error is smaller at 0.4770 compared to the MLR model's standard error of 0.6662. In other words the estimate of the LMM with compound symmetry variance-covariance is more precise. 

#### Part C
What is the estimated difference in mean cortisol levels (and SE) between the depressed and non-depressed groups. Interpret the finding in a sentence. You do not need to conduct inference. 

From table 4 (or 5, they are equivalent for the casecontrol variable) the estimate difference between the control group and the depressed group is 0.9211. This means that cortisol is 0.9211 units higher in the depressed group. This estimate is identical to the MLR model, but has a higher standard error of 0.7180 compared to the MLR model's standard error of 0.3846.

# Question 7

#### Part A
Fit a linear mixed model assuming categorical time and group with an AR(1) structure using method=REML (the default). Print out the correlation matrix and interpret the general patterns of the correlation in the errors for an individual. 
```{r, echo = F}

### START QUESTION 7 CODE ###

## START QUESTION 7 PART A CODE ##

lmm_ar1 <- gls(mean_cortisol ~ time + casecontrol, data = cort_data_long,
                    correlation = corAR1(form = ~ 1 | SubjectId))

lmm_ar1_sum <- summary(lmm_ar1)

broom.mixed::tidy(lmm_ar1, conf.int = TRUE) %>%
  mutate(p.value = format.pval(p.value)) %>%
  select(term,estimate,std.error,conf.low,conf.high,p.value) %>%
  kbl(digits = 4, booktabs = T, align = "cc",
      caption = "LMM AR(1) Var-Cov",
      col.names = c("Term","Estimate","Std.Error","95% Conf.Low",
                    "95% Conf. High","P-Value")) %>%
  kable_styling(latex_options = "HOLD_position")

```
Table 8 shows the fixed effects for the linear mixed effects model with AR(1) variance-covariance. 
```{r,echo = F}

lmm_ar1$modelStruct$corStruct

getVarCov(lmm_ar1_sum)

## END QUESTION 7 PART A CODE ##

```
The above output shows the correlation structure for the linear mixed model with AR(1) variance-covariance. 

#### Part B
Provide a table of mean differences from the 6th time period along with SE's of the differences. Interpret two of the coefficients. You do not need to conduct inference. 
```{r, echo = F}

## START QUESTION 7 PART B CODE ##
lmm_ar1_relev <- gls(mean_cortisol ~ time + casecontrol, 
                      data = cort_long_relevel, 
                      correlation = corCAR1(form = ~ 1 | SubjectId))

broom.mixed::tidy(lmm_ar1_relev, conf.int = TRUE) %>%
  select(term,estimate,std.error,conf.low,conf.high,p.value) %>%
  mutate(p.value = format.pval(p.value)) %>%
  kbl(digits = 4, booktabs = T, align = "cc",
      caption = "LMM AR(1) Var-Cov - Time 6 Ref.",
      col.names = c("Term","Estimate","Std.Error","95% Conf.Low",
                    "95% Conf. High","P-Value")) %>%
  kable_styling(latex_options = "HOLD_position")

## END QUESTION 7 PART B CODE ##

### END QUESTION 7 CODE ###

```
Table 9 shows the output of the linear mixed model with AR(1) variance-covariance where time 6 is the reference group.

* For term timeTime1 this is the mean difference between cortisol at time 1 and cortisol at time 6. Interpreting this is means cortisol is 1.7518 units higher at time 6 compared to time 1. This estimate is identical to that of the MLR model from question 4, but the standard error is slightly smaller at 0.6542 compared to the MLR model's standard error of 0.6662. In other words the estimate of the LMM with AR(1) variance-covariance is more precise. 
* For term timeTime2 this is the mean difference between cortisol at time 2 and cortisol at time 6. Interpreting this it means cortisol is 2.2179 units higher at time 2 compared to time 6. This estimate is identical to that of the MLR model from question 4, but the standard error is slightly smaller at 0.6395 compared to the MLR model's standard error of 0.6662. In other words the estimate of LMM with AR(1) variance-covariance is slightly more precise. 

#### Part C
What is the estimated difference in mean cortisol levels (and SE) between the depressed and non-depressed groups. Interpret the finding in a sentence. You do not need to conduct inference. 

From table 4 (or 5, they are equivalent for the casecontrol variable) the estimate difference between the control group and the depressed group is 0.7446. This means that cortisol is 0.7446 units higher in the depressed group. This estimate is smaller compared to the MLR model, but has a higher standard error of 0.6096 compared to the MLR model's standard error of 0.3846.

# Question 8 
Write a paragraph comparing and contrasting the parameter estimates for $\beta$ (the mean differences) and the standard errors across the difference variance-covariance structures. 


```{r}
# Extracting covariance matrix from gls
#lmm <- gls()
#lmm_summ <- summary(lmm)
#varcov <- lmm_summ$modelStruct$corStruct

```


