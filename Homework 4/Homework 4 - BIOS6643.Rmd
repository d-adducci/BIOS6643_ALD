---
title: "Homework 4 - BIOS 6643 Analysis of Longitudinal Data"
author: "Dominic Adducci"
date: "2023-09-20"
output: html_document
---

```{r}

library(tidyverse)
library(latex2exp)

```

# Question 1
Complex MLE estimation in LMM requires computational optimization approaches, the goal here is to implement a basic Newton-Raphson algorithm in R. The following data are an i.i.d sample from a Cauchy($\theta$,1) distribution: 1.77, -0.23, 2.76, 3.80, 3.47, 56.75, -1.34, 4.24, -2.44, 3.29, 3.71, -2.40, 4.53, -0.07, -1.05, -13.87, -2.53, -1.75, 0.27, 43.21. 

#### Part A: 
Graph the log likelihood function. 

The likelihood function of the Cauchy($\theta$,1) distribution is: 
$$\frac{1}{\pi^n}\frac{1}{\prod[1+(x_i-\theta)^2]}$$
and the loglikelihood is:
$$-nlog(\pi)-\sum_i^nlog(1+(x_i - \theta)^2)$$

```{r}

### START QUESTION 1 CODE ###

## START QUESTION 1 PART A CODE ##

# Initializing experimental data. 
exp_data <- c(1.77,-0.23,2.76,3.80,3.47,56.75,-1.34,4.24,-2.44,3.29,3.71,-2.40,
              4.53,-0.07,-1.05,-13.87,-2.53,-1.75,0.27,43.21)

# Making a function to calculate the loglikelihood for the Cauchy distribution
# which has scale parameter set at 1. The location parameter is allowed to vary.
cauchy_function <- function(exp_data,theta){
  ll_prob <- -length(exp_data)*log(pi) - sum(log(1+(exp_data-theta)^2))
  output <- c(theta,ll_prob)
  return(output)
}

# Finding the median of the experimental data to give an idea of range for 
# potential location parameters (comes out to 1.02). 
cauchy_median <- median(exp_data)

# Initializing range of location parameters to check. Because Cauchy is 
# continuous thetas increase at increments of 1/30. Median of experimental 
# data is 1 so I'll include a range from -10 to 10. 
theta_range <- seq(-20,20,1/30)

# Using an apply statement to enter location parameter range into function.
ll_cauchy <- data.frame(t(sapply(theta_range, 
                               function(x) cauchy_function(exp_data,x))))

colnames(ll_cauchy) <- c("Theta","LogLikelihood")

# Selecting max loglikelihood value.
max_ll <- ll_cauchy[ll_cauchy$LogLikelihood == max(ll_cauchy$LogLikelihood),]

# Making a plot of the loglikelihood. 
ll_cauchy_plot <- ggplot(ll_cauchy, aes(x = Theta,
                                        y = LogLikelihood)) +
  geom_point(size = 0.1) +
  geom_vline(xintercept = max_ll[,1], color = "red") +
  geom_hline(yintercept = max_ll[,2], color = "red") +
  labs(title = TeX("LogLikelihood Cauchy($\\theta$,1)"),
       x = TeX("$\\theta$"), y = "LogLikelihood") +
  expand_limits(y = c(-125,-70)) +
  theme_bw()


ll_cauchy_plot

## END QUESTION 1 PART A CODE ##

```
From the plot of log-likelihood the maximum log-likelihood values is -72.92, and the optimal theta is -0.2. 

# Part B
Find (and write an R program) to find the MLE for $\theta$ using the Newton-Raphson method. 

The equation for the Newton-Raphson method in general form is as follows:
$$x_i = x_{i-1} - \frac{f(x_{i-1})}{f'(x_{i-1})}$$
Translating this to finding the MLE of $\theta$:
$$\hat{\theta}_{i}=\hat{\theta}_{i-1}-\frac{logL'(\hat{\theta}_{i-1})}{logL''(\hat{\theta}_{i-1})}=\hat{\theta}_{i-1}-\frac{S(\hat{\theta}_{i-1})}{I(\hat{\theta}_{i-1})}$$
What finding an MLE the score $S(\theta)$, the derivative of the log-likelihood, should equal 0, $S(\theta)=0$. $I(\theta)$ is the Fisher observed information from the data.

Check the code appendix for the function. 

```{r, echo = F}

## START QUESTION 1 PART B CODE

# For the log likelihood of the Cauchy distribution the first term drops out
# after taking the first derivative, so only the second needs to be included.
ll_cauchy_form <- expression(-log(1+(exp_data-theta)^2))

# Finding first and second derivatives of loglikelihood
# (will copy the output and put it into the function)
first_d <- D(ll_cauchy_form,"theta")
second_d <- D(first_d,"theta")

# Making a function to calculate the Newton-Raphson method.
nr_mle <- function(exp_data,theta_est,loglike,iterations){
  # Initializing a data frame to track progress
  theta_mat <- data.frame(matrix(NA,nrow = iterations,ncol=2))
  theta = theta_est # Initializing theta
  for(i in 1:iterations){
    # Calculating first and second derivative log likelihood
    ll_first_d <- sum(2 * (exp_data - theta)/(1 + (exp_data - theta)^2)) 
    ll_second_d <- sum(-(2/(1 + (exp_data - theta)^2) - 2 * 
                           (exp_data - theta) * 
                           (2 * (exp_data - theta))/
                           (1 + (exp_data - theta)^2)^2)) 
    theta <- theta - ll_first_d/ll_second_d # Using Newton-Raphson equation
    # Calculating difference from original theta. 
    difference <- theta - theta_est 
    # Adding these to the data frame. 
    theta_mat[i,1] <- theta
    theta_mat[i,2] <- difference
  }
  colnames(theta_mat) <- c("Estimate","Difference")
  return(theta_mat)
}


theta_mle <- nr_mle(exp_data,4,ll_cauchy_form,20)
theta_mle
```

#### Part C
Try all of the following starting points: -11,-1,0,1.5,4,4.7,7,8,and 38. 




# Question 2
In a paragraph explain the difference between a general linear model or multiple regression (GLM; not a generalized linear model like a logistic regression or (GLM; not a generalized linear model like a logistic regression or Poisson, which will be discussed later) and a linear mixed model (LMM). 

A general linear model only has fixed effects, while a linear mixed model includes both fixed and random effects. The random effects of the linear mixed model allows the model to account for differences between subjects. A simple example would be measuring something like cholesterol levels through time. A general linear model (fixed effects only) may include covariates such as time, BMI, smoking status, sex, race, etc. Every individual will follow the same regression line based on those covariates. In a linear mixed model random effect which account for subject differences, such as someone tending to have higher or lower cholesterol at start (random intercept) can be included to better model change over time. A better fit may be found by including random slopes as well for each subject if cholesterol trajectory is found to be different between subjects. 

# Question 3
In a short paragraph, explain the difference between a profiled likelihood and a restricted likelihood for a linear mixed model, and how and why they are used. 

In a profile likelihood you maximize the likelihood by fixing every other parameter and only allowing one to vary. Doing this maximizes the single parameter you allowed to vary. You can then repeat this process for every other parameter incrementally, plugging in the estimates for parameters which have already been maximized. The downside to this method is that variance estimates are biased downward. The restricted likelihood (REML) allows for estimating parameters which are not biased regardless of sample size. The downside for the REML method compared to profile likelihood is that you can only compare REML model using a likelihood ratio of both models have the same set of fixed effects. 

Profile likelihood - maximize likelihood by fixing every other parameter and only allow one to vary and maximize that using a grid search. After getting that maximum repeat each process for each parameter in the set of likelihood parameter.

Restricted maximum likelihood - REML has property that your standard error are unbiased regardless of sample size, so generally is in situations where were trying to get unbiased estimates of errors when sample sizes are small. Loglikelihood are not valid for reduced in full REML models. Can only compare REML models that have the same set of fixed effects. 













```{r}
# Extracting covariance matrix from gls
lmm <- gls()
lmm_summ <- summary(lmm)
varcov <- lmm_summ$modelStruct$corStruct

```


